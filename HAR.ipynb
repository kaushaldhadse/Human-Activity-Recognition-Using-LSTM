{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFlair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Recognition\n",
    "\n",
    "Activities:\n",
    "Sitting,\n",
    "Standing,\n",
    "Walking,\n",
    "Upstairs,\n",
    "Downstairs\n",
    "\n",
    "Sensors:\n",
    "Accelerometer,\n",
    "Gyroscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signal_Types = [   #These are all the distinct types of signals in the dataset\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating functions to load X_train, X_test, y_train and y_test from the dataset\n",
    "\n",
    "def get_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "\n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "\n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "\n",
    "def get_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]],\n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "\n",
    "    \n",
    "    return y_ - 1 # Subtracting 1 for zero based indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining paths and using them to load the data into X_train, X_test, y_train and y_test from the dataset.\n",
    "\n",
    "Data_path = \"C:\\\\Python programming\\\\DataFlair\\\\HAR\\\\human+activity+recognition+using+smartphones\\\\UCI HAR Dataset\"\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    Data_path + \"\\\\train\" + \"\\\\Inertial Signals\\\\\" + signal + \"train.txt\" for signal in Signal_Types\n",
    "]\n",
    "\n",
    "X_test_signals_paths = [\n",
    "    Data_path + \"\\\\test\" + \"\\\\Inertial Signals\\\\\" + signal + \"test.txt\" for signal in Signal_Types\n",
    "]\n",
    "\n",
    "X_train = get_X(X_train_signals_paths)\n",
    "X_test = get_X(X_test_signals_paths)\n",
    "\n",
    "\n",
    "y_train_path = Data_path + \"\\\\train\\\\\" + \"y_train.txt\"\n",
    "y_test_path = Data_path + \"\\\\test\\\\\" + \"y_test.txt\"\n",
    "\n",
    "y_train = get_y(y_train_path)\n",
    "y_test = get_y(y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding y_train and y_test\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=6)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFlair\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 128, 64)           18944     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128, 64)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52358 (204.52 KB)\n",
      "Trainable params: 52358 (204.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "230/230 [==============================] - 27s 104ms/step - loss: 0.9189 - accuracy: 0.6045 - val_loss: 0.6740 - val_accuracy: 0.6919\n",
      "Epoch 2/20\n",
      "230/230 [==============================] - 23s 102ms/step - loss: 0.6320 - accuracy: 0.7609 - val_loss: 0.8260 - val_accuracy: 0.6641\n",
      "Epoch 3/20\n",
      "230/230 [==============================] - 27s 117ms/step - loss: 0.5489 - accuracy: 0.8064 - val_loss: 0.3766 - val_accuracy: 0.8656\n",
      "Epoch 4/20\n",
      "230/230 [==============================] - 33s 142ms/step - loss: 0.3404 - accuracy: 0.9002 - val_loss: 0.2851 - val_accuracy: 0.8935\n",
      "Epoch 5/20\n",
      "230/230 [==============================] - 30s 132ms/step - loss: 0.2595 - accuracy: 0.9157 - val_loss: 0.3167 - val_accuracy: 0.8853\n",
      "Epoch 6/20\n",
      "230/230 [==============================] - 32s 140ms/step - loss: 0.2911 - accuracy: 0.9072 - val_loss: 0.4528 - val_accuracy: 0.8582\n",
      "Epoch 7/20\n",
      "230/230 [==============================] - 34s 149ms/step - loss: 0.8371 - accuracy: 0.6721 - val_loss: 0.5820 - val_accuracy: 0.7513\n",
      "Epoch 8/20\n",
      "230/230 [==============================] - 39s 168ms/step - loss: 0.4037 - accuracy: 0.8538 - val_loss: 0.3566 - val_accuracy: 0.8704\n",
      "Epoch 9/20\n",
      "230/230 [==============================] - 29s 128ms/step - loss: 0.2945 - accuracy: 0.9082 - val_loss: 0.3114 - val_accuracy: 0.8972\n",
      "Epoch 10/20\n",
      "230/230 [==============================] - 29s 127ms/step - loss: 0.2501 - accuracy: 0.9188 - val_loss: 0.3014 - val_accuracy: 0.9063\n",
      "Epoch 11/20\n",
      "230/230 [==============================] - 31s 136ms/step - loss: 0.1682 - accuracy: 0.9404 - val_loss: 0.2758 - val_accuracy: 0.9040\n",
      "Epoch 12/20\n",
      "230/230 [==============================] - 33s 145ms/step - loss: 0.1681 - accuracy: 0.9438 - val_loss: 0.3071 - val_accuracy: 0.9036\n",
      "Epoch 13/20\n",
      "230/230 [==============================] - 32s 138ms/step - loss: 0.2138 - accuracy: 0.9319 - val_loss: 0.2999 - val_accuracy: 0.8979\n",
      "Epoch 14/20\n",
      "230/230 [==============================] - 30s 130ms/step - loss: 0.1757 - accuracy: 0.9414 - val_loss: 0.3019 - val_accuracy: 0.9094\n",
      "Epoch 15/20\n",
      "230/230 [==============================] - 33s 145ms/step - loss: 0.1664 - accuracy: 0.9422 - val_loss: 0.4717 - val_accuracy: 0.8687\n",
      "Epoch 16/20\n",
      "230/230 [==============================] - 31s 137ms/step - loss: 0.1614 - accuracy: 0.9425 - val_loss: 0.3053 - val_accuracy: 0.8921\n",
      "Epoch 17/20\n",
      "230/230 [==============================] - 32s 138ms/step - loss: 0.1417 - accuracy: 0.9459 - val_loss: 0.2921 - val_accuracy: 0.9050\n",
      "Epoch 18/20\n",
      "230/230 [==============================] - 32s 137ms/step - loss: 0.1918 - accuracy: 0.9378 - val_loss: 0.3771 - val_accuracy: 0.8951\n",
      "Epoch 19/20\n",
      "230/230 [==============================] - 31s 134ms/step - loss: 0.1620 - accuracy: 0.9446 - val_loss: 0.2700 - val_accuracy: 0.9135\n",
      "Epoch 20/20\n",
      "230/230 [==============================] - 30s 133ms/step - loss: 0.1339 - accuracy: 0.9512 - val_loss: 0.2994 - val_accuracy: 0.9077\n",
      "93/93 [==============================] - 4s 41ms/step - loss: 0.2994 - accuracy: 0.9077\n",
      "Test Loss: 0.2994, Test Accuracy: 0.9077\n"
     ]
    }
   ],
   "source": [
    "# Building and traiing the LSTM model.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, input_shape=(128, 9), return_sequences=True))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(LSTM(units=64))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(units=6, activation='softmax'))\n",
    "\n",
    "# Compiling the model\n",
    "optimizer = Adam(learning_rate=0.0025)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"DataFlair\")\n",
    "\n",
    "# Printing the model summary\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train_one_hot, epochs=20, batch_size=32, validation_data=(X_test, y_test_one_hot))\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
